{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://space.bilibili.com/272226120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import TransformerEncoderLayer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NaiveTransformerLayer, self).__init__()\n",
    "        self.dim = 768\n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.fnn1 = nn.Linear(self.dim, self.dim * 4, bias=True)\n",
    "        self.fnn2 = nn.Linear(self.dim * 4, self.dim * 1, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "        \n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input n*d\n",
    "        output n*d\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        attention_score = torch.mm(Q, K.transpose(0, 1)) / math.sqrt(self.dim)\n",
    "        attention_score = nn.Softmax(dim=-1)(attention_score)\n",
    "        O = torch.mm(attention_score, V)\n",
    "        O = self.lm(x + O)\n",
    "        return O\n",
    "    def FFN(self, x):\n",
    "        t1 = self.fnn1(x)\n",
    "        t1 = self.act(t1)\n",
    "        t2 = self.fnn2(t1)\n",
    "        output = self.lm_ffn(t2 + x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x n*d\n",
    "        # output n*d\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 768]), torch.Size([4, 768]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 =torch.rand(size=(4,768))\n",
    "# output1= NaiveTransformerLayer()(sample1)\n",
    "net =NaiveTransformerLayer()\n",
    "output1= net(sample1)\n",
    "sample1.shape,output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## naivetransformerlayer 问题\n",
    "    ## 1 batch\n",
    "    ## 2 dropout\n",
    "    ## 3 multihead\n",
    "    ## 4 attention mask ,padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add batch , drop out\n",
    "class Batch_NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Batch_NaiveTransformerLayer, self).__init__()\n",
    "        self.dim = 768\n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.fnn1 = nn.Linear(self.dim, self.dim * 4, bias=True)\n",
    "        self.fnn2 = nn.Linear(self.dim * 4, self.dim * 1, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "        self.att_drop_prob = 0.1\n",
    "        self.state_drop_prob = 0.1\n",
    "        self.att_drop = nn.Dropout(self.att_drop_prob)\n",
    "        self.state_drop = nn.Dropout(self.state_drop_prob)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input n*d\n",
    "        output n*d\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        attention_score = torch.bmm(Q, K.transpose(-2, -1)) / math.sqrt(self.dim)\n",
    "        attention_score = nn.Softmax(dim=-1)(attention_score)\n",
    "        attention_score =self.att_drop(attention_score)\n",
    "        O = torch.bmm(attention_score, V)\n",
    "        O= self.state_drop(O)\n",
    "        O = self.lm(x + O)\n",
    "        return O\n",
    "\n",
    "    def FFN(self, x):\n",
    "        t1 = self.fnn1(x)\n",
    "        t1 = self.act(t1)\n",
    "        t2 = self.fnn2(t1)\n",
    "        t2 =self.state_drop(t2)\n",
    "        output = self.lm_ffn(t2 + x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x n*d\n",
    "        # output n*d\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 4, 768]), torch.Size([32, 4, 768]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 =torch.rand(size=(32,4,768))\n",
    "# output1= NaiveTransformerLayer()(sample1)\n",
    "net =Batch_NaiveTransformerLayer()\n",
    "output1= net(sample1)\n",
    "sample1.shape,output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add multihead_attention\n",
    "class MH_NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MH_NaiveTransformerLayer, self).__init__()\n",
    "        self.dim = 768\n",
    "        self.num_heads = 12\n",
    "        self.per_head_size = self.dim // self.num_heads  ## 64\n",
    "        self.Wq = nn.Linear(self.dim, self.num_heads * self.per_head_size, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.num_heads * self.per_head_size, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.num_heads * self.per_head_size, bias=False)\n",
    "        self.W = nn.Linear(self.num_heads*self.per_head_size,self.dim) ## 使用这个线性映射来保证多头后还能映射到原来的维度\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.fnn1 = nn.Linear(self.dim, self.dim * 4, bias=True)\n",
    "        self.fnn2 = nn.Linear(self.dim * 4, self.dim * 1, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "        self.att_drop_prob = 0.1\n",
    "        self.state_drop_prob = 0.5\n",
    "        self.att_drop = nn.Dropout(self.att_drop_prob)\n",
    "        self.state_drop = nn.Dropout(self.state_drop_prob)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input batch_size * N*(Head_num*Head_size) ==>rearray ==> batch_size * N*Head_num*Head_size==>rearray ==>batch_size * Head_num * N * Head_size\n",
    "        output batch_size* n * d\n",
    "        \"\"\"\n",
    "        # torch.Size([32, 4, 12, 64])\n",
    "        # x.size 返回的是torch.Size,这是tuple的字类,两个tuple合并使用的是 +\n",
    "        new_size = x.size()[:-1] + (self.num_heads, self.per_head_size)  # B* N* H*S\n",
    "        Q = self.Wq(x).view(*new_size).permute(0,2,1,3)\n",
    "        K = self.Wk(x).view(*new_size).permute(0,2,1,3)\n",
    "        V = self.Wv(x).view(*new_size).permute(0,2,1,3)\n",
    "        attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dim)\n",
    "        attention_score = nn.Softmax(dim=-1)(attention_score)\n",
    "        attention_score = self.att_drop(attention_score)\n",
    "        O = torch.matmul(attention_score, V)\n",
    "        O = self.W(O.permute(0, 2, 1, 3).contiguous().view(x.size(0), x.size(1), -1)) #  #O#:B* H* N* S==>B* N* H* S==> bxnxd\n",
    "        O = self.state_drop(O)\n",
    "        O = self.lm(x + O)\n",
    "        return O\n",
    "\n",
    "    def FFN(self, x):\n",
    "        t1 = self.fnn1(x)\n",
    "        t1 = self.act(t1)\n",
    "        t2 = self.fnn2(t1)\n",
    "        t2 = self.state_drop(t2)\n",
    "        output = self.lm_ffn(t2 + x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x n*d\n",
    "        # output n*d\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 4, 768]), torch.Size([32, 4, 768]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 =torch.rand(size=(32,4,768))\n",
    "# output1= NaiveTransformerLayer()(sample1)\n",
    "net =MH_NaiveTransformerLayer()\n",
    "output1= net(sample1)\n",
    "sample1.shape,output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add mask\n",
    "## add multihead_attention\n",
    "class Mask_TransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mask_TransformerLayer, self).__init__()\n",
    "        self.dim = 768\n",
    "        self.num_heads = 12\n",
    "        self.per_head_size = self.dim // self.num_heads  ## 64\n",
    "        self.Wq = nn.Linear(self.dim, self.num_heads * self.per_head_size, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.num_heads * self.per_head_size, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.num_heads * self.per_head_size, bias=False)\n",
    "        self.W = nn.Linear(\n",
    "            self.num_heads * self.per_head_size, self.dim\n",
    "        )  ## 使用这个线性映射来保证多头后还能映射到原来的维度\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.fnn1 = nn.Linear(self.dim, self.dim * 4, bias=True)\n",
    "        self.fnn2 = nn.Linear(self.dim * 4, self.dim * 1, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "        self.att_drop_prob = 0.1\n",
    "        self.state_drop_prob = 0.5\n",
    "        self.att_drop = nn.Dropout(self.att_drop_prob)\n",
    "        self.state_drop = nn.Dropout(self.state_drop_prob)\n",
    "\n",
    "    def calc_mask_score(self, attention_mask):\n",
    "        \"\"\"\n",
    "        input bxn\n",
    "        output b*h*n*n 对于任意一个head 中,那些点会被注意到 n*n\n",
    "        \"\"\"\n",
    "        mask_score = torch.zeros(\n",
    "            attention_mask.size(0),\n",
    "            self.num_heads,\n",
    "            attention_mask.size(1),\n",
    "            attention_mask.size(1),\n",
    "        )\n",
    "        mask_score = mask_score + attention_mask[:, None, None, :]\n",
    "        mask_score = (1.0 - mask_score) * -10000.\n",
    "        return mask_score\n",
    "\n",
    "    def SelfAttention(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        input batch_size * N*(Head_num*Head_size) ==>rearray ==> batch_size * N*Head_num*Head_size==>rearray ==>batch_size * Head_num * N * Head_size\n",
    "        output batch_size* n * d\n",
    "\n",
    "        attention mask  batchsize*N  ,N 序列长度\n",
    "        1 normal token\n",
    "        0 masked token\n",
    "        \"\"\"\n",
    "        # torch.Size([32, 4, 12, 64])\n",
    "        # x.size 返回的是torch.Size,这是tuple的字类,两个tuple合并使用的是 +\n",
    "        new_size = x.size()[:-1] + (self.num_heads, self.per_head_size)  # B* N* H*S\n",
    "        Q = self.Wq(x).view(*new_size).permute(0, 2, 1, 3)\n",
    "        K = self.Wk(x).view(*new_size).permute(0, 2, 1, 3)\n",
    "        V = self.Wv(x).view(*new_size).permute(0, 2, 1, 3)\n",
    "        attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dim)\n",
    "        ## attention mask here\n",
    "        mask_score = attention_score + self.calc_mask_score(attention_mask)\n",
    "\n",
    "        attention_score = nn.Softmax(dim=-1)(attention_score)\n",
    "        attention_score = self.att_drop(attention_score)\n",
    "        O = torch.matmul(attention_score, V)\n",
    "        O = self.W(\n",
    "            O.permute(0, 2, 1, 3).contiguous().view(x.size(0), x.size(1), -1)\n",
    "        )  #  #O#:B* H* N* S==>B* N* H* S==> bxnxd\n",
    "        O = self.state_drop(O)\n",
    "        O = self.lm(x + O)\n",
    "        return O\n",
    "\n",
    "    def FFN(self, x):\n",
    "        t1 = self.fnn1(x)\n",
    "        t1 = self.act(t1)\n",
    "        t2 = self.fnn2(t1)\n",
    "        t2 = self.state_drop(t2)\n",
    "        output = self.lm_ffn(t2 + x)\n",
    "        return output\n",
    "\n",
    "    def forward(self,x,attention_mask):\n",
    "        # input x n*d\n",
    "        # output n*d\n",
    "        x = self.SelfAttention(x,attention_mask)\n",
    "        x = self.FFN(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 768]), torch.Size([2, 256, 768]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 =torch.rand(size=(2, 256, 768)) # batchsize* N* Embedding_size\n",
    "# output1= NaiveTransformerLayer()(sample1)\n",
    "mask = torch.ones(2, 256)\n",
    "net =Mask_TransformerLayer()\n",
    "output1= net(sample1,mask)\n",
    "sample1.shape,output1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8729,  1.0437,  0.9460,  ...,  0.7100,  0.8173, -1.1307],\n",
       "         [-2.5397, -0.3485, -1.0852,  ..., -1.0142,  0.6679,  1.7684],\n",
       "         [-0.3519, -0.9706, -0.4831,  ..., -0.8022,  1.4280,  0.7184],\n",
       "         ...,\n",
       "         [ 1.4107,  0.8165,  0.2495,  ..., -0.0963, -0.3215, -0.2316],\n",
       "         [-0.5850,  1.0678, -0.1695,  ...,  0.1663, -0.3970, -0.0920],\n",
       "         [-0.9799, -0.1537, -0.6260,  ..., -1.2784,  0.4568, -0.5208]],\n",
       "\n",
       "        [[ 0.2158,  0.4168, -0.0114,  ..., -0.9031,  0.5463, -0.3733],\n",
       "         [-2.5492, -1.2238, -0.8947,  ...,  0.4318,  0.6822, -0.7082],\n",
       "         [-0.8914, -0.8931, -0.3195,  ..., -1.1099, -0.1089, -1.0964],\n",
       "         ...,\n",
       "         [-0.5354, -1.4312,  0.6768,  ..., -0.3599, -0.5681, -0.5770],\n",
       "         [-0.7852, -1.1449,  0.2709,  ...,  0.6154, -0.1276, -0.9554],\n",
       "         [-0.1394, -1.2817, -1.6374,  ...,  0.9248,  1.3273, -0.0487]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e71f074aeebc52c11099cf58ff9227b7cedfec9164a950fb481d4785c5ef552a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
